# -*- coding: utf-8 -*-
"""modle_lernv2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ERGMYqoFyK5ZfQHVF4xvqbSOQ2r_4R7D
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2, l1
from tensorflow.keras.losses import Huber, LogCosh
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load the data from Google Drive
file_path = '/content/drive/My Drive/summerschool/data_modv6.csv'

data = pd.read_csv(file_path, sep=',')  # Assuming the file is tab-separated, adjust accordingly

# Identify numerical and categorical columns
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = data.select_dtypes(include=['object']).columns.tolist()

# Remove target columns from feature columns lists
numerical_cols.remove('home_club_goals')
numerical_cols.remove('away_club_goals')

# Preprocessing for numerical data
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Prepare features and target variables
X = data.drop(columns=['home_club_goals', 'away_club_goals'])
y = data[['home_club_goals', 'away_club_goals']]

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess the data
X_train = preprocessor.fit_transform(X_train).toarray()
X_test = preprocessor.transform(X_test).toarray()

#----------------------------------------------------------------------------------------------------------------

# Function to create and train a model with given hyperparameters
def train_model(params):
    activation, kernel_regularizer, loss, optimizer, dropout = params
    # Define the neural network model
    model = Sequential()
    model.add(Dense(512, input_dim=X_train.shape[1], activation=activation, kernel_regularizer=kernel_regularizer))
    model.add(Dropout(dropout))
    model.add(Dense(256, activation=activation, kernel_regularizer=kernel_regularizer))
    model.add(Dropout(dropout))
    model.add(Dense(128, activation=activation, kernel_regularizer=kernel_regularizer))
    model.add(Dropout(dropout))
    model.add(Dense(64, activation=activation, kernel_regularizer=kernel_regularizer))
    model.add(Dropout(dropout))
    model.add(Dense(32, activation=activation, kernel_regularizer=kernel_regularizer))
    model.add(Dense(2, dtype='float32'))  # Output layer for home and away goals

    # Compile the model with a different learning rate
    model.compile(optimizer=optimizer, loss=loss, metrics=['mae'])

    # Define early stopping
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Train the model
    history = model.fit(X_train, y_train, epochs=100, batch_size=256, validation_split=0.2, callbacks=[early_stopping])

    # Save the model
    model_name = f'model_{activation}_{kernel_regularizer}_{loss}_{optimizer}_{dropout}.keras'
    model_path = f'/content/drive/My Drive/summerschool/models2/{model_name}'
    model.save(model_path)

    # Evaluate the model
    loss, mae = model.evaluate(X_test, y_test, verbose=0)
    y_pred = model.predict(X_test, verbose=0)
    win_accuracy, loss_accuracy, draw_accuracy, overall_accuracy = evaluate_predictions(y_test.values, y_pred)

    return (model_path, win_accuracy, loss_accuracy, draw_accuracy, overall_accuracy)

def evaluate_predictions(true_results, predicted_results):
    correct_wins = 0
    correct_losses = 0
    correct_draws = 0
    total_wins = 0
    total_losses = 0
    total_draws = 0

    # Predicions rounder, for evaluation purpeses.
    #predicted_results = np.round(predicted_results)

    for i in range(len(true_results)):
        true_home, true_away = true_results[i]
        pred_home, pred_away = predicted_results[i]

        true_outcome = 'win' if true_home > true_away else 'loss' if true_home < true_away else 'draw'
        pred_outcome = 'win' if pred_home > pred_away else 'loss' if pred_home < pred_away else 'draw'

        if true_outcome == 'win':
            total_wins += 1
            if true_outcome == pred_outcome:
                correct_wins += 1
        elif true_outcome == 'loss':
            total_losses += 1
            if true_outcome == pred_outcome:
                correct_losses += 1
        elif true_outcome == 'draw':
            total_draws += 1
            if true_outcome == pred_outcome:
                correct_draws += 1

    total_correct = correct_wins + correct_losses + correct_draws
    total_predictions = len(true_results)

    # Calculate percentages
    win_accuracy = (correct_wins / total_wins) * 100 if total_wins > 0 else 0
    loss_accuracy = (correct_losses / total_losses) * 100 if total_losses > 0 else 0
    draw_accuracy = (correct_draws / total_draws) * 100 if total_draws > 0 else 0
    overall_accuracy = (total_correct / total_predictions) * 100

    return win_accuracy, loss_accuracy, draw_accuracy, overall_accuracy

# Define the combinations of hyperparameters
kernel_regularizers = [l2(0.001)]
activation_functions = ['relu']
dropout_values = [0.3]
optimizers = [Adam(learning_rate=0.0001), SGD(learning_rate=0.001), RMSprop(learning_rate=0.0001)]
loss_functions = ['mean_absolute_error']# 'mean_absolute_error', 'mean_squared_error','huber'




# Train models sequentially
results = []
for kernel_regularizer in kernel_regularizers:
    for activation_function in activation_functions:
        for dropout_value in dropout_values:
            for loss_function in loss_functions:
                for optimizer in optimizers:
                    parameters = (activation_function, kernel_regularizer, loss_function, optimizer, dropout_value)
                    result = train_model(parameters)
                    results.append(result)


print(results)